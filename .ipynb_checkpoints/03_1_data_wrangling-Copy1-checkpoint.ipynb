{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d881a14d-1884-43a5-a7a3-f9bfd7f227b1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>NOTE</b> \n",
    "\n",
    "<div>\n",
    "    This is a duplicate of notebook <b>03_02_no_temporal_data_wrangling.ipynb </b>\n",
    "    in this notebook I want to split the data into training/validation//testing sets.\n",
    "    I'll use an 80 (training/validation)//20 (testing) split\n",
    "</div>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cc6c3e2-ec89-4a38-9515-92ccde674182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from auxiliary_functions import write_joblib\n",
    "from data_wrangling import load_all_pickles, remove_small_dfs, remove_undesired_columns, remove_remaining_data, length_total, combined_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fc6b1c3-9b40-484c-b11c-28463090a5f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/luisescobar/Documents/Thesis/DataSets/Dictionary/02_Clotting_Labelling\u001b[39m\u001b[38;5;124m'\u001b[39m \n\u001b[1;32m      3\u001b[0m path_to_save1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/luisescobar/Documents/Thesis/DataSets/Dictionary/03_01_Train_Val_Test\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m loaded_pickles \u001b[38;5;241m=\u001b[39m load_all_pickles(folder_path)\n\u001b[1;32m      6\u001b[0m no_clotting_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      7\u001b[0m clotting_dict \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/Documents/Thesis/Code/GitHub/data_wrangling.py:27\u001b[0m, in \u001b[0;36mload_all_pickles\u001b[0;34m(folder_path)\u001b[0m\n\u001b[1;32m     25\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, pkl_file)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 27\u001b[0m         pickles[pkl_file] \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pickles\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "# Load dictionaries \n",
    "folder_path = '/Users/luisescobar/Documents/Thesis/DataSets/Dictionary/02_Clotting_Labelling' \n",
    "path_to_save1 = '/Users/luisescobar/Documents/Thesis/DataSets/Dictionary/03_01_Train_Val_Test'\n",
    "loaded_pickles = load_all_pickles(folder_path)\n",
    "\n",
    "no_clotting_dict = {}\n",
    "clotting_dict = {}\n",
    "\n",
    "# Iterate through the original dictionary and sort based on the key\n",
    "for key, value in loaded_pickles.items():\n",
    "    if \"no_clotting\" in key:\n",
    "        no_clotting_dict[key] = value\n",
    "    elif \"clotting\" in key:\n",
    "        clotting_dict[key] = value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e98e1ab-8189-4d17-b76a-54168a406325",
   "metadata": {},
   "source": [
    "Now I want to run the quality control. I need to iterate through each .pkl file. Each file is composed of multiple time series; I want to discard those whose len(time series) < 70. I would like to print the len of each dictionary after running this test    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b60e26a-c355-43b0-a98f-a232644a7c55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns=['Date__Heure', 'trt', 'Patient_weight__Kg_' , 'Set', 'Condition_1', 'Condition_2', 'Delta_P_ref', 'TMP_ref', 'Clotting_1', 'group']\n",
    "min_length = 70\n",
    "shift = 15  # number of points we want to conserve once we identify a blocking event \n",
    "# Filter for removing small Data Frames (length)\n",
    "remove_small_dfs(no_clotting_dict, min_length)\n",
    "remove_small_dfs(clotting_dict, min_length)\n",
    "\n",
    "#Filter for removing undesired columns\n",
    "remove_undesired_columns(no_clotting_dict,columns)\n",
    "remove_undesired_columns(clotting_dict,columns)\n",
    "\n",
    "#Filter for cutting, in the case of blocking data, the elements after the blocking event\n",
    "remove_remaining_data(clotting_dict)\n",
    "#remove_undesired_data_custom(clotting_dict, shift)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08165a73-bcd7-4cb0-a49f-032bf25f2646",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>Note regarding remove_undesired_data_custom() function.</b> \n",
    "\n",
    "With the results obtained so far, it seems that this function is obsolete since the model performs better when we add as many blocking points as possible. This is easily done with the function remove_undesired_columns()\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cd8aa5-4731-4c1d-a56f-990a44a4f9e0",
   "metadata": {},
   "source": [
    "Lets see how many data frames from each class do we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46ea6549-bd97-40c1-8484-abcb5d5387cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completo1007_(edit)_clotting.pkl 18\n",
      "completo600_(edit)_clotting.pkl 15\n",
      "completo400_(edit)_clotting.pkl 19\n",
      "completo_800_output_file_clotting.pkl 25\n",
      "completo200_(edit)_clotting.pkl 14\n"
     ]
    }
   ],
   "source": [
    "length_total(clotting_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "431dc89c-911f-4155-9b4d-ac2b8f62f1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completo400_(edit)_no_clotting.pkl 121\n",
      "completo600_(edit)_no_clotting.pkl 155\n",
      "completo1007_(edit)_no_clotting.pkl 157\n",
      "completo200_(edit)_no_clotting.pkl 141\n",
      "completo_800_output_file_no_clotting.pkl 131\n"
     ]
    }
   ],
   "source": [
    "length_total(no_clotting_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c45cc72-d83a-4105-969f-fe02f1bf94e5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "1. I want to merge, randomly, all the timeseries of each dictionary into a global one. At the end of this process I'll have blocking_dict and no_blocking_dict\n",
    "2. I want to split each global dictionary into two subsets 90/10 so I can (train,validate)/test. Therefore, I'll have\n",
    "   blocking_dict_tval\n",
    "   blocking_dict_test\n",
    "   no_blocking_dict_tval\n",
    "   no_blocking_dict_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c5e1425-3a05-485e-8d56-b7f6d92126b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "global_dict_block = {**combined_items(clotting_dict)}\n",
    "items_block = list(global_dict_block.items())\n",
    "random.shuffle(items_block)\n",
    "global_dict_block = dict(items_block)\n",
    "\n",
    "global_dict_no_block = {**combined_items(no_clotting_dict)}\n",
    "items_no_block = list(global_dict_no_block.items())\n",
    "random.shuffle(items_no_block)\n",
    "global_dict_no_block = dict(items_no_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a91422e-6597-47c1-9739-47ce6bd25f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "705\n"
     ]
    }
   ],
   "source": [
    "print(len(global_dict_block))\n",
    "print(len(global_dict_no_block))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d5e8f80-bcfd-4c0e-bd19-315e5c620871",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_keys_block = list(global_dict_block.keys())\n",
    "split_index_block = int(len(global_dict_block) * 0.8)\n",
    "keys_90_percent_block = random.sample(total_keys_block, split_index_block)\n",
    "dict_90_block = {key: global_dict_block[key] for key in keys_90_percent_block}\n",
    "keys_10_percent_block = list(set(total_keys_block) - set(keys_90_percent_block))\n",
    "dict_10_block = {key: global_dict_block[key] for key in keys_10_percent_block}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5fb8e7e-08ec-41de-9573-12869d04df19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "print(len(dict_90_block))\n",
    "print(len(dict_10_block))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fed6b4b-457b-4f58-92eb-d8410abb4c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_keys_no_block = list(global_dict_no_block.keys())\n",
    "split_index_no_block = int(len(global_dict_no_block) * 0.8)\n",
    "keys_90_percent_no_block = random.sample(total_keys_no_block, split_index_no_block)\n",
    "dict_90_no_block = {key: global_dict_no_block[key] for key in keys_90_percent_no_block}\n",
    "keys_10_percent_no_block = list(set(total_keys_no_block) - set(keys_90_percent_no_block))\n",
    "dict_10_no_block = {key: global_dict_no_block[key] for key in keys_10_percent_no_block}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91d613e0-2a6d-43ed-8859-16f097f0a81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "564\n",
      "141\n"
     ]
    }
   ],
   "source": [
    "print(len(dict_90_no_block))\n",
    "print(len(dict_10_no_block))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29eb56c5-343e-4d3f-a2e8-6548dd19d21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionaries\n",
    "name_save = f'{path_to_save1}/blocking_80.pkl' \n",
    "with open(name_save, 'wb') as file:\n",
    "    pickle.dump(dict_90_block, file)\n",
    "\n",
    "write_joblib(dict_80_block)\n",
    "\n",
    "name_save = f'{path_to_save1}/blocking_20.pkl' \n",
    "with open(name_save, 'wb') as file:\n",
    "    pickle.dump(dict_10_block, file)\n",
    "\n",
    "\n",
    "name_save = f'{path_to_save1}/no_blocking_80.pkl' \n",
    "with open(name_save, 'wb') as file:\n",
    "    pickle.dump(dict_90_no_block, file)\n",
    "\n",
    "name_save = f'{path_to_save1}/no_blocking_20.pkl' \n",
    "with open(name_save, 'wb') as file:\n",
    "    pickle.dump(dict_10_no_block, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5f8606-c76b-4326-9d52-1f0cf9bd647a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
