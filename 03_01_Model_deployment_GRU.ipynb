{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aca40b20-f9aa-4a18-9f12-d01a351784e8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example dictionary (patient data stored as Pandas DataFrames)\n",
    "data = {\n",
    "    \"patient_1\": pd.DataFrame({\n",
    "        'x1': [1.0, 2.0, 3.0],\n",
    "        'x2': [2.0, 3.0, 4.0],\n",
    "        'x3': [1.0, 2.0, 1.5],\n",
    "        'x4': [0.5, 0.7, 0.9],\n",
    "        'y': [0, 1, 0]\n",
    "    }),\n",
    "    \"patient_2\": pd.DataFrame({\n",
    "        'x1': [2.0, 3.0],\n",
    "        'x2': [1.0, 2.0],\n",
    "        'x3': [0.5, 0.7],\n",
    "        'x4': [0.3, 0.6],\n",
    "        'y': [1, 0]\n",
    "    }),\n",
    "    \"patient_3\": pd.DataFrame({\n",
    "        'x1': [1.0, 1.0, 2.0, 2.0],\n",
    "        'x2': [2.0, 3.0, 1.5, 3.0],\n",
    "        'x3': [1.0, 0.8, 1.2, 1.1],\n",
    "        'x4': [0.5, 0.7, 0.6, 0.8],\n",
    "        'y': [0, 1, 0, 1]\n",
    "    }),\n",
    "    # Add more patients as needed\n",
    "}\n",
    "\n",
    "# Step 1: Split the data into training and test sets\n",
    "patient_ids = list(data.keys())\n",
    "train_ids, test_ids = train_test_split(patient_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "train_data = {k: data[k] for k in train_ids}\n",
    "test_data = {k: data[k] for k in test_ids}\n",
    "\n",
    "# Step 2: Prepare the data for training\n",
    "def prepare_data(data_dict):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    lengths = []\n",
    "\n",
    "    for patient_id, df in data_dict.items():\n",
    "        # Extract explanatory variables (x1, x2, x3, x4)\n",
    "        sequence = torch.tensor(df[['x1', 'x2', 'x3', 'x4']].values, dtype=torch.float32)\n",
    "\n",
    "        # Extract the target variable (y)\n",
    "        label = torch.tensor(df['y'].values[0], dtype=torch.float32)  # Assuming consistent label across series\n",
    "\n",
    "        sequences.append(sequence)\n",
    "        labels.append(label)\n",
    "        lengths.append(len(sequence))\n",
    "\n",
    "    # Convert labels to a tensor\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    # Pad sequences to the maximum length\n",
    "    padded_sequences = nn.utils.rnn.pad_sequence(sequences, batch_first=True)\n",
    "\n",
    "    # Convert lengths to a tensor\n",
    "    lengths = torch.tensor(lengths)\n",
    "\n",
    "    return padded_sequences, labels, lengths\n",
    "\n",
    "# Prepare training and test data\n",
    "train_sequences, train_labels, train_lengths = prepare_data(train_data)\n",
    "test_sequences, test_labels, test_lengths = prepare_data(test_data)\n",
    "\n",
    "# Step 3: Define the GRU model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size=1):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        packed_input = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, _ = self.gru(packed_input)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        idx = (lengths - 1).view(-1, 1).expand(len(lengths), output.size(2)).unsqueeze(1)\n",
    "        last_output = output.gather(1, idx).squeeze(1)\n",
    "\n",
    "        return torch.sigmoid(self.fc(last_output))\n",
    "\n",
    "# Instantiate the model\n",
    "input_size = train_sequences.size(2)  # Number of features (4 in this case: x1, x2, x3, x4)\n",
    "hidden_size = 64  # Number of GRU units\n",
    "model = GRUModel(input_size, hidden_size)\n",
    "\n",
    "# Step 4: Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Step 5: Training Loop\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass for training\n",
    "    train_outputs = model(train_sequences, train_lengths)\n",
    "    loss = criterion(train_outputs.squeeze(), train_labels)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Step 6: Test the model (optional)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(test_sequences, test_lengths)\n",
    "    predictions = (test_outputs.squeeze() > 0.5).float()\n",
    "    print(f'Predictions: {predictions}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42804082-5eb7-40ca-a9a1-ac719dbcee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f462359f-be06-4324-bf1a-ddaf5c1a58b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read DataFrames from dictionary\n",
    "\n",
    "# Load the dictionary from the file\n",
    "path_to_read = '/Users/luisescobar/Documents/Thesis/DataSets/Dictionary'\n",
    "#path_to_save = '/Users/luisescobar/Documents/Thesis/DataSets/Dictionary'\n",
    "file_name = 'completo1007_(edit)_clotting.pkl'\n",
    "name_to_read = f'{path_to_read}/{file_name}'\n",
    "\n",
    "with open(name_to_read, 'rb') as file:\n",
    "    loaded_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1db83075-fd8d-45d0-9388-4d957e305557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dictionary: 148\n",
      "Test Dictionary: 37\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and test\n",
    "'''\n",
    "Maybe will be a better idea to split the final DataFrames into clotting and no clotting. Once we have this split we\n",
    "can divide in a better ratio for train and test. Plot a graph in the previous notebook to see the balance between classes   \n",
    "'''\n",
    "# Step 1: Split the data into training and test sets\n",
    "\n",
    "# Extract the keys\n",
    "keys = list(loaded_dict.keys())\n",
    "\n",
    "# Split the keys (80% train, 20% test)\n",
    "train_keys, test_keys = train_test_split(keys, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the train and test dictionaries\n",
    "train_dict = {key: loaded_dict[key] for key in train_keys}\n",
    "test_dict = {key: loaded_dict[key] for key in test_keys}\n",
    "\n",
    "print(\"Train Dictionary:\", len(train_dict))\n",
    "print(\"Test Dictionary:\", len(test_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410d2c8a-8764-4073-a14b-90c1cab9f415",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "columns=[\"Date__Heure\",\"P_Access\",\"P_Filter\",\"P_Effluent\",\"P_Return\",\"Q_Blood_Pump\",\n",
    "          \"Q_Replacement\", \"Q_Dialysate\", \"Q_PBP\", \"Q_Patient_Fluid_Removal\", \"DeltaP\", \"TMP\", \"TMPa\", \"trt\", \n",
    "         \"Patient_weight__Kg_\", \"Set\"]\n",
    "\n",
    "For training we could remove\n",
    "0   Date__Heure ---> maybe we could find seasonality with this variable. However, we must transform the data  \n",
    "13  trt \n",
    "14  Patient_weight__Kg_ ---> we could include it for other models to see if it produce a prediction difference\n",
    "15  Set \n",
    "'''\n",
    "columns=[\"P_Access\",\"P_Filter\",\"P_Effluent\",\"P_Return\",\"Q_Blood_Pump\",\n",
    "         \"Q_Replacement\", \"Q_Dialysate\", \"Q_PBP\", \"Q_Patient_Fluid_Removal\", \n",
    "         \"DeltaP\", \"TMP\", \"TMPa\", \"Condition_1\", \"Condition_2\", \"Delta_P_ref\", \n",
    "         \"TMP_ref\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc09d7c4-afeb-4f97-bf80-8a7e2af14f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare the data for training\n",
    "def prepare_data(data_dict):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    lengths = []\n",
    "\n",
    "    for patient_id, df in data_dict.items():\n",
    "        # Extract explanatory variables (x1, x2, x3, x4)\n",
    "        sequence = torch.tensor(df[['x1', 'x2', 'x3', 'x4']].values, dtype=torch.float32)\n",
    "\n",
    "        # Extract the target variable (y)\n",
    "        label = torch.tensor(df['y'].values[0], dtype=torch.float32)  # Assuming consistent label across series\n",
    "\n",
    "        sequences.append(sequence)\n",
    "        labels.append(label)\n",
    "        lengths.append(len(sequence))\n",
    "\n",
    "    # Convert labels to a tensor\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    # Pad sequences to the maximum length\n",
    "    padded_sequences = nn.utils.rnn.pad_sequence(sequences, batch_first=True)\n",
    "\n",
    "    # Convert lengths to a tensor\n",
    "    lengths = torch.tensor(lengths)\n",
    "\n",
    "    return padded_sequences, labels, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456ba160-50f8-407a-b64f-a2bd9f5b4120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training and test data\n",
    "train_sequences, train_labels, train_lengths = prepare_data(train_data)\n",
    "test_sequences, test_labels, test_lengths = prepare_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60afd91-5d98-4d50-9763-cdc981bc0d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define the GRU model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size=1):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        packed_input = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, _ = self.gru(packed_input)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        idx = (lengths - 1).view(-1, 1).expand(len(lengths), output.size(2)).unsqueeze(1)\n",
    "        last_output = output.gather(1, idx).squeeze(1)\n",
    "\n",
    "        return torch.sigmoid(self.fc(last_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672531c9-71bb-457b-9435-ef5556801111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "input_size = train_sequences.size(2)  # Number of features (4 in this case: x1, x2, x3, x4)\n",
    "hidden_size = 64  # Number of GRU units\n",
    "model = GRUModel(input_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43734083-f6e7-45b0-9252-b2e6dfbe127e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4caada-3d1e-4537-bf21-b19a3f89281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Training Loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass for training\n",
    "    train_outputs = model(train_sequences, train_lengths)\n",
    "    loss = criterion(train_outputs.squeeze(), train_labels)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2151aa0b-8479-4c39-8c77-daea03b3444e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Test the model (optional)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(test_sequences, test_lengths)\n",
    "    predictions = (test_outputs.squeeze() > 0.5).float()\n",
    "    print(f'Predictions: {predictions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479c9d9b-7088-4ecb-89c1-d99c8ff367d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02ded6da-c888-4279-a8d5-f01821cac2a3",
   "metadata": {},
   "source": [
    "Problems to address\n",
    "How dictionary will work during Step 1\n",
    "Should we normalize the values?\n",
    "Double check the step # Extract the target variable (y) since is assuming that the same label for the entire series\n",
    "What does this means?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
