{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cc6c3e2-ec89-4a38-9515-92ccde674182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33464b7c-6ed0-4b2b-b83a-2f82bb0a5f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_pickles(folder_path):\n",
    "    '''The function load all the pickle files from a given folder. It assumes that the folder only\n",
    "    contains .pkl files. If different extensions exist it will return an error\n",
    "    Parameters\n",
    "    ----------\n",
    "    folder_path : str\n",
    "        String with the path where the pkl files are saved i.e.,\n",
    "        '/Users/luisescobar/Documents/Thesis/DataSets/Dictionary/02_Clotting_Labelling'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pickles : dict\n",
    "        Dictionary since from the previous notebook 02_Clotting_Labelling data were saved in this format\n",
    "    '''\n",
    "    # List all files in the directory (assuming all are .pkl files)\n",
    "    all_files = os.listdir(folder_path)\n",
    "    \n",
    "    # Load each .pkl file and store the results in a dictionary\n",
    "    pickles = {}\n",
    "    for pkl_file in all_files:\n",
    "        file_path = os.path.join(folder_path, pkl_file)\n",
    "        with open(file_path, 'rb') as f:\n",
    "            pickles[pkl_file] = pickle.load(f)\n",
    "    \n",
    "    return pickles\n",
    "\n",
    "# Load dictionaries \n",
    "folder_path = '/Users/luisescobar/Documents/Thesis/DataSets/Dictionary/02_Clotting_Labelling'  \n",
    "loaded_pickles = load_all_pickles(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fc6b1c3-9b40-484c-b11c-28463090a5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_clotting_dict = {}\n",
    "clotting_dict = {}\n",
    "\n",
    "# Iterate through the original dictionary and sort based on the key\n",
    "for key, value in loaded_pickles.items():\n",
    "    if \"no_clotting\" in key:\n",
    "        no_clotting_dict[key] = value\n",
    "    elif \"clotting\" in key:\n",
    "        clotting_dict[key] = value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e98e1ab-8189-4d17-b76a-54168a406325",
   "metadata": {},
   "source": [
    "Now I want to run the quality control. I need to iterate through each .pkl file. Each file is composed of multiple time series; I want to discard those whose len(time series) < 40. I would like to print the len of each dictionary after running this test    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21339500-e01e-4dc7-ad69-09d7b69b199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove DataFrames with length < 70\n",
    "def remove_small_dfs(outer_dict, min_length):\n",
    "    '''The function returns only the Data Frames in which the length is >= 70\n",
    "    Parameters\n",
    "    ----------\n",
    "    outer_dict : dict \n",
    "        Dictionary of dictionaries. See the structure below\n",
    "            data_dict = {\n",
    "                        'inner_dict_1': {\n",
    "                            'df_trt_1': df1,\n",
    "                            'df_trt_2': df2\n",
    "                        },\n",
    "                        'inner_dict_2': {\n",
    "                            'df_trt_3': df3\n",
    "                        }\n",
    "                    }\n",
    "        Each one of the inner dictionaries contains a set of Data Frames, each Data Frame corresponds to a single treatment \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    outer_dict : dict\n",
    "        Dictionary with the same structure as the input but without those Data Frames whose length < min_length  \n",
    "    '''\n",
    "    for key in outer_dict:\n",
    "        inner_dict = outer_dict[key]\n",
    "        outer_dict[key] = {df_name: df for df_name, df in inner_dict.items() if len(df) >= min_length}\n",
    "\n",
    "\n",
    "def remove_undesired_columns(outer_dict,columns):\n",
    "    '''The function returns a new version of Data Frames in which we preserve only the columns of interest \n",
    "    Parameters\n",
    "    ----------\n",
    "    outer_dict : dict \n",
    "        Dictionary of dictionaries. See the structure below\n",
    "            data_dict = {\n",
    "                        'inner_dict_1': {\n",
    "                            'df_trt_1': df1,\n",
    "                            'df_trt_2': df2\n",
    "                        },\n",
    "                        'inner_dict_2': {\n",
    "                            'df_trt_3': df3\n",
    "                        }\n",
    "                    }\n",
    "        Each one of the inner dictionaries contains a set of Data Frames, each Data Frame corresponds to a single treatment \n",
    "\n",
    "     columns : list\n",
    "        List with the columns we want to remove\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    outer_dict : dict\n",
    "        Dictionary with the same structure as the input but in which each Data Frame has only the columns of interest\n",
    "        \n",
    "    '''    \n",
    "    for key in outer_dict:\n",
    "        inner_dict = outer_dict[key]\n",
    "        outer_dict[key] = {df_name: df.drop(columns=columns, errors='ignore') \n",
    "                           for df_name, df in inner_dict.items()\n",
    "                          }\n",
    "\n",
    "\n",
    "def remove_remaining_data(outer_dict):\n",
    "    '''The function returns a new version of Data Frames, only in the case of blocking/clotting, in which we cut all the time series data\n",
    "    after detecting the blocking/clotting event \n",
    "    Parameters\n",
    "    ----------\n",
    "    outer_dict : dict \n",
    "        Dictionary of dictionaries. See the structure below\n",
    "            data_dict = {\n",
    "                        'inner_dict_1': {\n",
    "                            'df_trt_1': df1,\n",
    "                            'df_trt_2': df2\n",
    "                        },\n",
    "                        'inner_dict_2': {\n",
    "                            'df_trt_3': df3\n",
    "                        }\n",
    "                    }\n",
    "        Each one of the inner dictionaries contains a set of Data Frames, each Data Frame corresponds to a single treatment         \n",
    "  \n",
    "    Returns\n",
    "    -------\n",
    "    outer_dict : dict\n",
    "        Dictionary with the same structure as the input but in which each Data Frame, from blocking/clotting, has cut all the time series data\n",
    "    after detecting the blocking/clotting event \n",
    "    '''\n",
    "    for key in outer_dict:\n",
    "        inner_dict = outer_dict[key]\n",
    "        outer_dict[key] = {df_name: df[df['Clotting_2'].ne(df['Clotting_2'].shift()).cumsum() <= 2]\n",
    "                           for df_name, df in inner_dict.items()\n",
    "                          }\n",
    "\n",
    "def length_total(dict_primal):\n",
    "    '''The function returns the length of the seconday dictionaries embedded on a primal dictionary\n",
    "    Parameters\n",
    "    ----------\n",
    "    dict_primal : dict \n",
    "        Dictionary of dictionaries. See the structure below\n",
    "            data_dict = {\n",
    "                        'inner_dict_1': {\n",
    "                            'df_trt_1': df1,\n",
    "                            'df_trt_2': df2\n",
    "                        },\n",
    "                        'inner_dict_2': {\n",
    "                            'df_trt_3': df3\n",
    "                        }\n",
    "                    }\n",
    "        Each one of the inner dictionaries contains a set of Data Frames, each Data Frame corresponds to a single treatment \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None : \n",
    "        \n",
    "    '''\n",
    "    for keys in list(dict_primal.keys()):\n",
    "        print(f'{keys} {len(dict_primal[keys].items())}')\n",
    "\n",
    "    \n",
    "def combined_items(dict_primal):\n",
    "    '''The function returns a dictionary combining the Data Frames corresponding to the seconday dictionaries \n",
    "    Parameters\n",
    "    ----------\n",
    "    dict_primal : dict \n",
    "        Dictionary of dictionaries. See the structure below\n",
    "            dict_primal = {\n",
    "                        'inner_dict_1': {\n",
    "                            'df_trt_1': df1,\n",
    "                            'df_trt_2': df2\n",
    "                        },\n",
    "                        'inner_dict_2': {\n",
    "                            'df_trt_3': df3\n",
    "                        }\n",
    "                    }\n",
    "        Each one of the inner dictionaries contains a set of Data Frames, each Data Frame corresponds to a single treatment \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    combined_dict : dict\n",
    "        Combined dictionary from dict_primal. See the structure below\n",
    "            combined_dict = {\n",
    "                            'df_trt_1': df1,\n",
    "                            'df_trt_2': df2\n",
    "                            'df_trt_3': df3\n",
    "                        }\n",
    "    '''\n",
    "    combined_dict = {}\n",
    "    for keys in list(dict_primal.keys()):\n",
    "        for name, df in dict_primal[keys].items():\n",
    "            combined_dict[name] = df\n",
    "    return combined_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b60e26a-c355-43b0-a98f-a232644a7c55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns=['Date__Heure', 'trt', 'Patient_weight__Kg_' , 'Set', 'Condition_1', 'Condition_2', 'Delta_P_ref', 'TMP_ref', 'Clotting_1', 'group']\n",
    "min_length = 70\n",
    "\n",
    "# Filter for removing small Data Frames (length)\n",
    "remove_small_dfs(no_clotting_dict, min_length)\n",
    "remove_small_dfs(clotting_dict, min_length)\n",
    "\n",
    "#Filter for removing undesired columns\n",
    "remove_undesired_columns(no_clotting_dict,columns)\n",
    "remove_undesired_columns(clotting_dict,columns)\n",
    "\n",
    "#Filter for cutting, in the case of blocking data, the elements after the blocking event\n",
    "remove_remaining_data(clotting_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cd8aa5-4731-4c1d-a56f-990a44a4f9e0",
   "metadata": {},
   "source": [
    "Lets see how many data frames from each class do we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46ea6549-bd97-40c1-8484-abcb5d5387cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completo1007_(edit)_clotting.pkl 18\n",
      "completo600_(edit)_clotting.pkl 15\n",
      "completo400_(edit)_clotting.pkl 19\n",
      "completo_800_output_file_clotting.pkl 25\n",
      "completo200_(edit)_clotting.pkl 14\n"
     ]
    }
   ],
   "source": [
    "length_total(clotting_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "431dc89c-911f-4155-9b4d-ac2b8f62f1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completo400_(edit)_no_clotting.pkl 121\n",
      "completo600_(edit)_no_clotting.pkl 155\n",
      "completo1007_(edit)_no_clotting.pkl 157\n",
      "completo200_(edit)_no_clotting.pkl 141\n",
      "completo_800_output_file_no_clotting.pkl 131\n"
     ]
    }
   ],
   "source": [
    "length_total(no_clotting_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fafc0f6-0b81-4488-8931-96a6d90a8727",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine all the data into a single random shuffle dictionary \n",
    "random.seed(42)\n",
    "global_dict = {**combined_items(clotting_dict), **combined_items(no_clotting_dict)}\n",
    "items = list(global_dict.items())\n",
    "random.shuffle(items)\n",
    "global_dict = dict(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afae22fe-9a92-46cc-990d-f2cd2ba64867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "796"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(global_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "653a9303-34cb-4575-b7a0-2e3580bc7a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a single DataFrame ready to train\n",
    "dataframes_list = list(global_dict.values())\n",
    "\n",
    "# Concatenate all DataFrames vertically\n",
    "combined_df = pd.concat(dataframes_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "546ff039-0e20-4230-abef-ebca10e0dbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1586575 entries, 0 to 1586574\n",
      "Data columns (total 13 columns):\n",
      " #   Column                   Non-Null Count    Dtype  \n",
      "---  ------                   --------------    -----  \n",
      " 0   P_Access                 1586575 non-null  int64  \n",
      " 1   P_Filter                 1586575 non-null  int64  \n",
      " 2   P_Effluent               1586575 non-null  int64  \n",
      " 3   P_Return                 1586575 non-null  int64  \n",
      " 4   Q_Blood_Pump             1586575 non-null  int64  \n",
      " 5   Q_Replacement            1586575 non-null  int64  \n",
      " 6   Q_Dialysate              1586575 non-null  int64  \n",
      " 7   Q_PBP                    1586575 non-null  int64  \n",
      " 8   Q_Patient_Fluid_Removal  1586575 non-null  int64  \n",
      " 9   DeltaP                   1586575 non-null  int64  \n",
      " 10  TMP                      1586575 non-null  float64\n",
      " 11  TMPa                     1586575 non-null  int64  \n",
      " 12  Clotting_2               1586575 non-null  int64  \n",
      "dtypes: float64(1), int64(12)\n",
      "memory usage: 157.4 MB\n"
     ]
    }
   ],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5f8606-c76b-4326-9d52-1f0cf9bd647a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
