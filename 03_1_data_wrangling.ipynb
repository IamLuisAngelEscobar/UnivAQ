{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cc6c3e2-ec89-4a38-9515-92ccde674182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33464b7c-6ed0-4b2b-b83a-2f82bb0a5f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_pickles(folder_path):\n",
    "    '''The function load all the pickle files from a given folder. It assumes that the folder only\n",
    "    contains .pkl files. If different extensions exist it will return an error\n",
    "    Parameters\n",
    "    ----------\n",
    "    folder_path : str\n",
    "        String with the path where the pkl files are saved i.e.,\n",
    "        '/Users/luisescobar/Documents/Thesis/DataSets/Dictionary/02_Clotting_Labelling'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pickles : dict\n",
    "        Dictionary since from the previous notebook 02_Clotting_Labelling data were saved in this format\n",
    "    '''\n",
    "    # List all files in the directory (assuming all are .pkl files)\n",
    "    all_files = os.listdir(folder_path)\n",
    "    \n",
    "    # Load each .pkl file and store the results in a dictionary\n",
    "    pickles = {}\n",
    "    for pkl_file in all_files:\n",
    "        file_path = os.path.join(folder_path, pkl_file)\n",
    "        with open(file_path, 'rb') as f:\n",
    "            pickles[pkl_file] = pickle.load(f)\n",
    "    \n",
    "    return pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fc6b1c3-9b40-484c-b11c-28463090a5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dictionaries \n",
    "folder_path = '/Users/luisescobar/Documents/Thesis/DataSets/Dictionary/02_Clotting_Labelling' \n",
    "path_to_save1 = '/Users/luisescobar/Documents/Thesis/DataSets/Dictionary/03_Combined_dict'\n",
    "path_to_save2 = '/Users/luisescobar/Documents/Thesis/DataSets/Dictionary/03_01_Sepatate_dict'\n",
    "loaded_pickles = load_all_pickles(folder_path)\n",
    "\n",
    "no_clotting_dict = {}\n",
    "clotting_dict = {}\n",
    "\n",
    "# Iterate through the original dictionary and sort based on the key\n",
    "for key, value in loaded_pickles.items():\n",
    "    if \"no_clotting\" in key:\n",
    "        no_clotting_dict[key] = value\n",
    "    elif \"clotting\" in key:\n",
    "        clotting_dict[key] = value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e98e1ab-8189-4d17-b76a-54168a406325",
   "metadata": {},
   "source": [
    "Now I want to run the quality control. I need to iterate through each .pkl file. Each file is composed of multiple time series; I want to discard those whose len(time series) < 40. I would like to print the len of each dictionary after running this test    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21339500-e01e-4dc7-ad69-09d7b69b199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove DataFrames with length < 70\n",
    "def remove_small_dfs(outer_dict, min_length):\n",
    "    '''The function returns only the Data Frames in which the length is >= 70\n",
    "    Parameters\n",
    "    ----------\n",
    "    outer_dict : dict \n",
    "        Dictionary of dictionaries. See the structure below\n",
    "            data_dict = {\n",
    "                        'inner_dict_1': {\n",
    "                            'df_trt_1': df1,\n",
    "                            'df_trt_2': df2\n",
    "                        },\n",
    "                        'inner_dict_2': {\n",
    "                            'df_trt_3': df3\n",
    "                        }\n",
    "                    }\n",
    "        Each one of the inner dictionaries contains a set of Data Frames, each Data Frame corresponds to a single treatment \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    outer_dict : dict\n",
    "        Dictionary with the same structure as the input but without those Data Frames whose length < min_length  \n",
    "    '''\n",
    "    for key in outer_dict:\n",
    "        inner_dict = outer_dict[key]\n",
    "        outer_dict[key] = {df_name: df for df_name, df in inner_dict.items() if len(df) >= min_length}\n",
    "\n",
    "\n",
    "def remove_undesired_columns(outer_dict,columns):\n",
    "    '''The function returns a new version of Data Frames in which we preserve only the columns of interest \n",
    "    Parameters\n",
    "    ----------\n",
    "    outer_dict : dict \n",
    "        Dictionary of dictionaries. See the structure below\n",
    "            data_dict = {\n",
    "                        'inner_dict_1': {\n",
    "                            'df_trt_1': df1,\n",
    "                            'df_trt_2': df2\n",
    "                        },\n",
    "                        'inner_dict_2': {\n",
    "                            'df_trt_3': df3\n",
    "                        }\n",
    "                    }\n",
    "        Each one of the inner dictionaries contains a set of Data Frames, each Data Frame corresponds to a single treatment \n",
    "\n",
    "     columns : list\n",
    "        List with the columns we want to remove\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    outer_dict : dict\n",
    "        Dictionary with the same structure as the input but in which each Data Frame has only the columns of interest\n",
    "        \n",
    "    '''    \n",
    "    for key in outer_dict:\n",
    "        inner_dict = outer_dict[key]\n",
    "        outer_dict[key] = {df_name: df.drop(columns=columns, errors='ignore') \n",
    "                           for df_name, df in inner_dict.items()\n",
    "                          }\n",
    "\n",
    "\n",
    "def remove_remaining_data(outer_dict):\n",
    "    '''The function returns a new version of Data Frames, only in the case of blocking/clotting, in which we cut all the time series data\n",
    "    after detecting the blocking/clotting event \n",
    "    Parameters\n",
    "    ----------\n",
    "    outer_dict : dict \n",
    "        Dictionary of dictionaries. See the structure below\n",
    "            data_dict = {\n",
    "                        'inner_dict_1': {\n",
    "                            'df_trt_1': df1,\n",
    "                            'df_trt_2': df2\n",
    "                        },\n",
    "                        'inner_dict_2': {\n",
    "                            'df_trt_3': df3\n",
    "                        }\n",
    "                    }\n",
    "        Each one of the inner dictionaries contains a set of Data Frames, each Data Frame corresponds to a single treatment         \n",
    "  \n",
    "    Returns\n",
    "    -------\n",
    "    outer_dict : dict\n",
    "        Dictionary with the same structure as the input but in which each Data Frame, from blocking/clotting, has cut all the time series data\n",
    "    after detecting the blocking/clotting event \n",
    "    '''\n",
    "    for key in outer_dict:\n",
    "        inner_dict = outer_dict[key]\n",
    "        outer_dict[key] = {df_name: df[df['Clotting_2'].ne(df['Clotting_2'].shift()).cumsum() <= 2]\n",
    "                           for df_name, df in inner_dict.items()\n",
    "                          }\n",
    "\n",
    "def length_total(dict_primal):\n",
    "    '''The function returns the length of the seconday dictionaries embedded on a primal dictionary \n",
    "    Parameters\n",
    "    ----------\n",
    "    dict_primal : dict \n",
    "        Dictionary of dictionaries. See the structure below\n",
    "            data_dict = {\n",
    "                        'inner_dict_1': {\n",
    "                            'df_trt_1': df1,\n",
    "                            'df_trt_2': df2\n",
    "                        },\n",
    "                        'inner_dict_2': {\n",
    "                            'df_trt_3': df3\n",
    "                        }\n",
    "                    }\n",
    "        Each one of the inner dictionaries contains a set of Data Frames, each Data Frame corresponds to a single treatment \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None : \n",
    "        \n",
    "    '''\n",
    "    for keys in list(dict_primal.keys()):\n",
    "        print(f'{keys} {len(dict_primal[keys].items())}')\n",
    "\n",
    "    \n",
    "def combined_items(dict_primal):\n",
    "    '''The function returns a dictionary combining the Data Frames corresponding to the seconday dictionaries \n",
    "    Parameters\n",
    "    ----------\n",
    "    dict_primal : dict \n",
    "        Dictionary of dictionaries. See the structure below\n",
    "            dict_primal = {\n",
    "                        'inner_dict_1': {\n",
    "                            'df_trt_1': df1,\n",
    "                            'df_trt_2': df2\n",
    "                        },\n",
    "                        'inner_dict_2': {\n",
    "                            'df_trt_3': df3\n",
    "                        }\n",
    "                    }\n",
    "        Each one of the inner dictionaries contains a set of Data Frames, each Data Frame corresponds to a single treatment \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    combined_dict : dict\n",
    "        Combined dictionary from dict_primal. See the structure below\n",
    "            combined_dict = {\n",
    "                            'df_trt_1': df1,\n",
    "                            'df_trt_2': df2\n",
    "                            'df_trt_3': df3\n",
    "                        }\n",
    "    '''\n",
    "    combined_dict = {}\n",
    "    for keys in list(dict_primal.keys()):\n",
    "        for name, df in dict_primal[keys].items():\n",
    "            combined_dict[name] = df\n",
    "    return combined_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1af5415e-6fc3-446b-a18e-6cf5b2545af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_undesired_data_custom(outer_dict, shift):\n",
    "    for key in outer_dict:\n",
    "        inner_dict = outer_dict[key]\n",
    "        outer_dict[key] = {df_name: df.iloc[:df[df['Clotting_2'] == 1].index[0] + shift]\n",
    "                           for df_name, df in inner_dict.items()\n",
    "                          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b60e26a-c355-43b0-a98f-a232644a7c55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns=['Date__Heure', 'trt', 'Patient_weight__Kg_' , 'Set', 'Condition_1', 'Condition_2', 'Delta_P_ref', 'TMP_ref', 'Clotting_1', 'group']\n",
    "min_length = 70\n",
    "shift = 15  # number of points we want to conserve once we identify a blocking event \n",
    "# Filter for removing small Data Frames (length)\n",
    "remove_small_dfs(no_clotting_dict, min_length)\n",
    "remove_small_dfs(clotting_dict, min_length)\n",
    "\n",
    "#Filter for removing undesired columns\n",
    "remove_undesired_columns(no_clotting_dict,columns)\n",
    "remove_undesired_columns(clotting_dict,columns)\n",
    "\n",
    "#Filter for cutting, in the case of blocking data, the elements after the blocking event\n",
    "remove_remaining_data(clotting_dict)\n",
    "#remove_undesired_data_custom(clotting_dict, shift)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08165a73-bcd7-4cb0-a49f-032bf25f2646",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>NOTE regarding remove_undesired_data_custom() function.</b> With the results obtained so far, it seems that this function is obsolete since the model performs better when we add as many blocking points as possible. This is easily done with the function remove_undesired_columns(). </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cd8aa5-4731-4c1d-a56f-990a44a4f9e0",
   "metadata": {},
   "source": [
    "Lets see how many data frames from each class do we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46ea6549-bd97-40c1-8484-abcb5d5387cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completo1007_(edit)_clotting.pkl 18\n",
      "completo600_(edit)_clotting.pkl 15\n",
      "completo400_(edit)_clotting.pkl 19\n",
      "completo_800_output_file_clotting.pkl 25\n",
      "completo200_(edit)_clotting.pkl 14\n"
     ]
    }
   ],
   "source": [
    "length_total(clotting_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "431dc89c-911f-4155-9b4d-ac2b8f62f1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completo400_(edit)_no_clotting.pkl 121\n",
      "completo600_(edit)_no_clotting.pkl 155\n",
      "completo1007_(edit)_no_clotting.pkl 157\n",
      "completo200_(edit)_no_clotting.pkl 141\n",
      "completo_800_output_file_no_clotting.pkl 131\n"
     ]
    }
   ],
   "source": [
    "length_total(no_clotting_dict)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9ef7314d-247c-49c5-be01-9ffbb7f7cd28",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Use this cell if you need to save the dictionaries separately\n",
    "\n",
    "name_clot = f'{path_to_save2}/clean_blocking_dict.pkl' \n",
    "with open(name_clot, 'wb') as file:\n",
    "    pickle.dump(clotting_dict, file)\n",
    "\n",
    "name_no_clot = f'{path_to_save2}/clean_no_blocking_dict.pkl' \n",
    "with open(name_no_clot, 'wb') as file:\n",
    "    pickle.dump(no_clotting_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cecb81d-aaa7-4c64-acea-d784e6622cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['df_801', 'df_802', 'df_806', 'df_807', 'df_808', 'df_809', 'df_810', 'df_811', 'df_812', 'df_813', 'df_816', 'df_817', 'df_818', 'df_819', 'df_820', 'df_821', 'df_822', 'df_823', 'df_824', 'df_826', 'df_827', 'df_828', 'df_829', 'df_830', 'df_832', 'df_833', 'df_835', 'df_836', 'df_839', 'df_840', 'df_842', 'df_843', 'df_844', 'df_845', 'df_846', 'df_849', 'df_850', 'df_851', 'df_853', 'df_854', 'df_855', 'df_856', 'df_857', 'df_858', 'df_860', 'df_861', 'df_862', 'df_863', 'df_868', 'df_869', 'df_870', 'df_871', 'df_872', 'df_874', 'df_875', 'df_876', 'df_877', 'df_878', 'df_879', 'df_880', 'df_881', 'df_882', 'df_883', 'df_884', 'df_885', 'df_886', 'df_887', 'df_888', 'df_889', 'df_891', 'df_894', 'df_895', 'df_896', 'df_897', 'df_898', 'df_899', 'df_900', 'df_901', 'df_903', 'df_904', 'df_905', 'df_906', 'df_907', 'df_908', 'df_910', 'df_914', 'df_918', 'df_920', 'df_921', 'df_924', 'df_925', 'df_926', 'df_928', 'df_929', 'df_930', 'df_933', 'df_935', 'df_936', 'df_937', 'df_939', 'df_940', 'df_941', 'df_943', 'df_944', 'df_945', 'df_946', 'df_947', 'df_948', 'df_950', 'df_951', 'df_952', 'df_953', 'df_954', 'df_955', 'df_956', 'df_957', 'df_958', 'df_960', 'df_961', 'df_962', 'df_963', 'df_964', 'df_965', 'df_966', 'df_967', 'df_968', 'df_974', 'df_975', 'df_978', 'df_979', 'df_980', 'df_981', 'df_982', 'df_983', 'df_984', 'df_985', 'df_986', 'df_987', 'df_991', 'df_992', 'df_994', 'df_995', 'df_996', 'df_997', 'df_998', 'df_1002', 'df_1003', 'df_1004', 'df_804-df_805-concat', 'df_837-df_838-concat', 'df_847-df_848-concat', 'df_864-df_865-df_866-df_867-concat', 'df_916-df_917-concat', 'df_970-df_971-concat', 'df_972-df_973-concat', 'df_988-df_989-df_990-concat', 'df_1005-df_1006-df_1007-concat'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_clotting_dict['completo1007_(edit)_no_clotting.pkl'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb705353-73dd-49d0-a7a0-a595989f4dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clotting_dict.keys()\n",
    "no_clotting_dict['completo1007_(edit)_no_clotting.pkl']['df_802']['Clotting_2'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fafc0f6-0b81-4488-8931-96a6d90a8727",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine all the data into a single random shuffle dictionary \n",
    "random.seed(42)\n",
    "global_dict = {**combined_items(clotting_dict), **combined_items(no_clotting_dict)}\n",
    "items = list(global_dict.items())\n",
    "random.shuffle(items)\n",
    "global_dict = dict(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afae22fe-9a92-46cc-990d-f2cd2ba64867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "796"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(global_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83dc628c-bfcd-4283-b511-914b469d610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save combined dictionary \n",
    "name_clot = f'{path_to_save1}/blocking_no_blocking_shift_15.pkl' \n",
    "with open(name_clot, 'wb') as file:\n",
    "    pickle.dump(global_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5f8606-c76b-4326-9d52-1f0cf9bd647a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
